# -*- coding: utf-8 -*-
"""Model_Training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eJDWm6rTGySaovd2YjwUdoIYBmnsyFDF

#**Initial Setup**
"""

!nvidia-smi

from google.colab import drive

 drive.mount('/content/drive')

# Extracting the dataset into the drive

import zipfile

with zipfile.ZipFile('/content/drive/MyDrive/Face_Physiognomy/FER_Dataset_NDF.zip', 'r') as zip_ref:
    zip_ref.extractall('/content/drive/MyDrive/Face_Physiognomy/')

# Importing all the required libraries

!pip install livelossplot

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import datetime
import os

import tensorflow as tf
from tensorflow import keras
from livelossplot import PlotLossesKerasTF
from keras.preprocessing.image import load_img, img_to_array
from keras.preprocessing.image import ImageDataGenerator
from keras.layers import Dense,Input,Dropout,GlobalAveragePooling2D,Flatten,Conv2D,BatchNormalization,Activation,MaxPooling2D
from keras.models import Model,Sequential
from tensorflow.keras.optimizers import Adam,SGD,RMSprop
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau

"""#**Exploring Data**"""

#Defining the Parameters and Hyperparameters

EXPRESSION = 'angry'
FOLDER_PATH = "/content/drive/MyDrive/Face_Physiognomy/images/"

PICTURE_SIZE = 48
BATCH_SIZE  = 64
NO_OF_CLASSES = 5
EPOCHS = 50

#Visualizing the Images of different classes

def plot_images(expression, picture_size, folder_path):
  plt.figure(figsize= (12,12))
  for i in range(1, 10, 1):
      plt.subplot(3,3,i)
      img = load_img(folder_path + "train/" + expression + "/" +
                    os.listdir(folder_path + "train/" + expression)[i], target_size=(picture_size, picture_size))
      plt.axis("off")
      plt.imshow(img)   
  
  plt.show()

plot_images(EXPRESSION, PICTURE_SIZE, FOLDER_PATH)

"""#**Image Generators**"""

#Creating Image Generators for Train and Validation Data

datagen_train = ImageDataGenerator(horizontal_flip=True)

datagen_valid = ImageDataGenerator(horizontal_flip=True)


train_set = datagen_train.flow_from_directory(folder_path + "train",
                                              target_size = (PICTURE_SIZE,PICTURE_SIZE),
                                              color_mode = "grayscale",
                                              batch_size = BATCH_SIZE,
                                              class_mode = 'categorical',
                                              shuffle = True)


valid_set = datagen_valid.flow_from_directory(folder_path + "validation",
                                              target_size = (PICTURE_SIZE,PICTURE_SIZE),
                                              color_mode = "grayscale",
                                              batch_size = BATCH_SIZE,
                                              class_mode = 'categorical',
                                              shuffle = False)

valid_set.classes

valid_set.class_indices.keys()

"""#**Model**

###**Architecture**
"""

#Defining the Model Architecture

model = Sequential()

#1st CNN layer
model.add(Conv2D(64,(3,3),padding = 'same',input_shape = (48,48,1)))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size = (2,2)))
model.add(Dropout(0.25))

#2nd CNN layer
model.add(Conv2D(128,(5,5),padding = 'same'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size = (2,2)))
model.add(Dropout (0.25))

#3rd CNN layer
model.add(Conv2D(512,(3,3),padding = 'same'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size = (2,2)))
model.add(Dropout (0.25))

#4th CNN layer
model.add(Conv2D(512,(3,3), padding = 'same'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size = (2, 2)))
model.add(Dropout(0.25))

model.add(Flatten())

#Fully connected 1st layer
model.add(Dense(256))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.25))


# Fully connected layer 2nd layer
model.add(Dense(512))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.25))

model.add(Dense(no_of_classes, activation = 'softmax'))


model.summary()

"""###**Compile**"""

#Defining callbacks for better training

checkpoint = ModelCheckpoint("/content/drive/MyDrive/Face_Physiognomy/best_model.h5", monitor = 'val_accuracy', verbose = 1, 
                             save_best_only = True, mode = 'max')

reduce_learning_rate = ReduceLROnPlateau(monitor='val_loss',
                              factor = 0.2,
                              patience = 6,
                              verbose = 1,
                              min_delta = 0.00001)

callbacks_list = [PlotLossesKerasTF(), checkpoint,reduce_learning_rate]

model.compile(loss='categorical_crossentropy',
              optimizer = Adam(learning_rate = 0.0005),
              metrics=['accuracy'])

"""###**Train**"""

#Training the Model

history = model.fit_generator(generator = train_set,
                              steps_per_epoch = train_set.n // train_set.batch_size,
                              epochs = epochs,
                              validation_data = valid_set,
                              validation_steps = valid_set.n // valid_set.batch_size,
                              callbacks = callbacks_list)

#Plotting the Loss and Accuracy 

plt.figure(figsize = (20,10))

plt.subplot(1, 2, 1)
plt.suptitle('Optimizer : Adam', fontsize = 20)
plt.ylabel('Loss', fontsize = 16)
plt.plot(history.history['loss'], label = 'Training Loss')
plt.plot(history.history['val_loss'], label = 'Validation Loss')
plt.legend(loc = 'upper right')

plt.subplot(1, 2, 2)
plt.ylabel('Accuracy', fontsize = 16)
plt.plot(history.history['accuracy'], label = 'Training Accuracy')
plt.plot(history.history['val_accuracy'], label = 'Validation Accuracy')
plt.legend(loc = 'lower right')
plt.show()

"""#**Visualizing Results and Reports**"""

model.evaluate(valid_set)

from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
import plotly.figure_factory as ff


test_steps_per_epoch = np.math.ceil(valid_set.samples / valid_set.batch_size)
predictions = model.predict(valid_set, steps=test_steps_per_epoch)

predicted_classes = np.argmax(predictions, axis=1)
true_classes = valid_set.classes
class_labels = list(valid_set.class_indices.keys())
report = classification_report(true_classes, predicted_classes, target_names = class_labels)
print(report)   

cm = confusion_matrix(true_classes,predicted_classes)

x = ['angry', 'happy', 'neutral', 'sad', 'surprise']
y = ['surprise', 'sad', 'neutral', 'happy', 'angry']

# change each element of cm to type string for annotations
z_text = [[str(round(y/len(predictions)*100, 2)) + '%' for y in x] for x in cm]

# set up figure 
fig = ff.create_annotated_heatmap(cm, x=x, y=y, annotation_text=z_text, colorscale='Viridis')

# add title
fig.update_layout(title_text='<i><b>Confusion matrix</b></i>',
                  # xaxis = dict(title='x'),
                  yaxis = dict(title='Real Class')
                 )

# add custom xaxis title
fig.add_annotation(dict(font=dict(color="black",size=14),
                        x=0.5,
                        y=-0.15,
                        showarrow=False,
                        text="Predicted Class",
                        xref="paper",
                        yref="paper"))

# add custom yaxis title
fig.add_annotation(dict(font=dict(color="black",size=14),
                        x=-0.35,
                        y=0.5,
                        showarrow=False,
                        textangle=-90,
                        xref="paper",
                        yref="paper"))

# adjust margins to make room for yaxis title
fig.update_layout(margin=dict(t=50, l=200))

# add colorbar
fig['data'][0]['showscale'] = True
fig.show()

"""#**Inferencing**"""

# Load Model Architecture

with open('/content/model.json', 'r') as json_file:
    json_savedModel= json_file.read()

model_j = tf.keras.models.model_from_json(json_savedModel)
model_j.summary()

#Load Model Weights

model_j.load_weights('/content/drive/MyDrive/Face_Physiognomy/best_model.h5')

# Inferencing

from google. colab. patches import cv2_imshow
import cv2

c = {0:'angry', 1:'happy', 2:'neutral', 3:'sad', 4:'surprise'}

img = cv2.imread('/content/happy.jpg', 0)
img_display = cv2.resize(img, (100,100))
print(img.shape)

faceCascade = cv2.CascadeClassifier('/content/haarcascade_frontalface_default.xml')
faces = faceCascade.detectMultiScale(img, 1.3, 5)
       
for (x, y, w, h) in faces: 
  fc = img[y:y+h, x:x+w]
  roi = cv2.resize(fc, (48, 48))
  roi = roi.reshape(1,48,48,1)
  x = np.argmax(model_j.predict(roi))

print(c.get(x))
cv2_imshow(img_display)

import glob
files = glob.glob("/content/drive/MyDrive/Face_Physiognomy/Validate/*")
print(files)

for f in files:
  img = cv2.imread(f, 0)
  img_display = cv2.resize(img, (100,100))
  print(img.shape)

  faceCascade = cv2.CascadeClassifier('/content/haarcascade_frontalface_default.xml')
  faces = faceCascade.detectMultiScale(img, 1.3, 5)
        
  for (x, y, w, h) in faces: 
    fc = img[y:y+h, x:x+w]
    roi = cv2.resize(fc, (48, 48))
    roi = roi.reshape(1,48,48,1)
    x = np.argmax(model_j.predict(roi))

  print(c.get(x))
  cv2_imshow(img_display)

# Save Model Architecture

tf.keras.utils.plot_model(
    model,
    to_file="model.png",
    show_shapes=False,
    show_dtype=False,
    show_layer_names=True,
    rankdir="TB",
    expand_nested=False,
    dpi=96,
    layer_range=None,
)